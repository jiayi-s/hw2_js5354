P8105 HW2 js5354
================
Jiayi Shen
September 28, 2018

Problem 1
=========

Loading the NYC Transit Subway Entrance and Exit Data:

``` r
#Load and clean the NYC Transit Subway Entrance and Exit Data

transit_data = 
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_integer(),
    ##   Route9 = col_integer(),
    ##   Route10 = col_integer(),
    ##   Route11 = col_integer(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
transit_data$entry = recode(transit_data$entry, "YES" = TRUE, "NO" =FALSE)
```

This CSV file contains information related to some subway stations in NYC, including which line each station is for, the name of each station, the geographic coordinates of each station as well as its entrance(s), what is the N-S and W-E street that this station is located at, which route(s) each station serves, whether there is free crossover / vending entries / stuffing / ADA compliance facilitated with any station, whether the station has any entrance or it is exit only as well as the type of entrance.
So far the csv file has been loaded and stored to `transit_data`; each column name has been transformed into lowercase letters with underscores in between; information with respect to line, station, name, station coordinates, routes served, entry, vending, entrance type, and ADA compliance have been retained. Then type of `entry` variables has been coerced into logical variables.
Now the dataset contains 1868 entries of 19 variables. However, the dataset is not tidy enough. Especially there are a lot of "NA"s in column `route1:11`, which unnecessarily widens the data spreadsheet.

Question 1
----------

Identifying number of stations:

``` r
#identifying number of stations
n_stations = nrow(distinct(transit_data, line, station_name))
```

There are 465 distinct stations in this NYC transit dataset.

Question 2
----------

Identifying the number of stations with ADA:

``` r
#identifying the number of stations with ADA
transit_data %>% 
  filter(ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow()
```

    ## [1] 84

84 stations are ADA compliant.

Question 3
----------

Identifying proportion of station entrances / exits without vending allow entrance:

``` r
#identifying proportion of station entrances / exits without vending allow entrance
transit_data %>% 
  filter (vending == "NO", entry == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow() / n_stations
```

    ## [1] 0.09247312

``` r
transit_data %>% 
  filter (vending == "NO", entry == FALSE) %>% 
  distinct(line, station_name) %>% 
  nrow() / n_stations
```

    ## [1] 0.1462366

There are 9.24% of total stations in this dataset have entrances without vending allow entrance and 14.62% have exit-only doors without vending.

Question 4
----------

Reformat data so that route number and route name are distinct variables.

``` r
#reformat to separate route number and route name
reformed_data = transit_data %>% 
  gather(key = "route_number", value = "route_name", route1:route11)

#identifying the number distinct stations serve the A train.
reformed_data %>% 
  filter (route_name == "A") %>% 
  distinct(line, station_name) %>% 
  nrow() 
```

    ## [1] 60

``` r
#identifying the number distinct stations serve the A train with ADA compliance.
reformed_data %>% 
  filter (route_name == "A", ada == TRUE) %>% 
  distinct(line, station_name) %>% 
  nrow() 
```

    ## [1] 17

There are 60 stations serve the A train, of which 17 stations are ADA compliant.

Problem 2
=========

Read and clean the Mr. Trash Wheel sheet:

``` r
#reading Excel file and stroing into R environment
trash_data = readxl::read_xlsx("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", range = "Mr. Trash Wheel!A2:N258") %>% 
  janitor::clean_names() 

#omit rows that do not include dumpster-specific data
trash_data = trash_data[!is.na(trash_data$dumpster), ]

#rounds the number of sports balls to the nearest integer and converts the result to an integer variable 
trash_data =
  trash_data %>% 
  mutate(sports_balls = round(sports_balls, digits = 0)) %>% 
  mutate(sports_balls = as.integer(sports_balls))
```

Read and clean precipitation data for 2016 and 2017.

``` r
#read precipitation data for 2016 and 2017 and omit rows without precipitation data

precipitation_16 = 
  readxl::read_xlsx("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", range = "2016 Precipitation!A2:B14") %>% 
  na.omit() 

precipitation_17 = 
  readxl::read_xlsx("./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx", range = "2017 Precipitation!A2:B14") %>% 
  na.omit()


#add a variable year
precipitation_16 ["year"] = 2016
precipitation_17 ["year"] = 2017

#combine datasets and convert month to a character variable
precipitation_combined = bind_rows(precipitation_16, precipitation_17) %>% 
  mutate(Month = month.name[Month])
```

The Mr. Trash Wheel sheet in the given Excel file contains information related to each trash collection event from May 2014 to August 2017 in a harbor area. For all 215 observations in Mr. Trash Wheel dataset, there listed information regarding the weight and volume of collected trash and its contents. Trash was categorized into plastic bottles, polystyrene, cigarette butts, glass bottles, grocery bags, chip bags and sports balls. The dataset also included the value (in number of homes powered) of how much electricity can be converted from the trash collected each time.
In average, there were 3.2764651 tons / 15.7069767 cubic yards of trash per collection.
The precipitation data for 2016 and 2017 summarised the monthly precipitation (in inches) in the two years. However, the values for 2017 September to December are not available given that this file was lastly updated on 2017-09-26. For the available precipitation data of 20 months, there was an average of 3.494 inches every month. The lowest precipitation (7.09 inches) took place in October 2016 whereas the heaviest was 7.09 inches in May 2017.  

For the available data, the total precipitation in 2017 was 29.93 till 2017-09-26.
The median number of sports balls in a dumpster in 2016 was 13

Problem 3
=========

``` r
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
#load data
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")

#tidy up dataset, including:
#focus on the “Overall Health” topic
#exclude variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation
#structure data so that responses (excellent to poor) are variables taking the value of Data_value

brfss_smart2010 = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter (topic == "Overall Health") %>% 
  select (-class, -topic, - question, -sample_size, -(confidence_limit_low:geo_location)) %>% 
  spread (key = response, value = data_value )%>% 
  janitor::clean_names()
  
#showing the proportion of responses that were “Excellent” or “Very Good”
prop_excellent = mean(brfss_smart2010$excellent, na.rm = TRUE)
prop_very_good = mean(brfss_smart2010$very_good, na.rm = TRUE)
```

``` r
#number of unique locations are included in the dataset
nrow(distinct(brfss_smart2010, locationdesc))
```

    ## [1] 404

``` r
#number of states present in the dataset
nrow(distinct(brfss_smart2010, locationabbr))
```

    ## [1] 51

``` r
#What state is observed the most?
count_(brfss_smart2010, 'locationabbr', sort = TRUE)
```

    ## # A tibble: 51 x 2
    ##    locationabbr     n
    ##    <chr>        <int>
    ##  1 NJ             146
    ##  2 FL             122
    ##  3 NC             115
    ##  4 WA              97
    ##  5 MD              90
    ##  6 MA              79
    ##  7 TX              71
    ##  8 NY              65
    ##  9 SC              63
    ## 10 CO              59
    ## # ... with 41 more rows

In this specific dataset, there are 404 unique locations, covering all 51 states in the US. The most observed state is NJ with 146 relevant entries.

``` r
#In 2002, what is the median of the “Excellent” response value?
brfss_2002 = filter(brfss_smart2010, year == 2002)
median(brfss_2002$excellent, na.rm = TRUE)
```

    ## [1] 23.6

``` r
#Make a histogram of “Excellent” response values in the year 2002.
ggplot(brfss_2002, aes(x = excellent)) + 
  geom_histogram()
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 2 rows containing non-finite values (stat_bin).

![](p8105_hw2_js5354_files/figure-markdown_github/questions%20related%20to%202002%20data-1.png)

``` r
#Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County in each year from 2002 to 2010.

#First, create dataset for further plotting
brfss_Q = brfss_smart2010 %>% 
  filter(locationdesc == "NY - Queens County")
brfss_NY = brfss_smart2010 %>% 
  filter(locationdesc == "NY - New York County")
brfss_combined = rbind(brfss_Q, brfss_NY)

#plot
ggplot(brfss_combined, aes(x = year, y = excellent, color = locationdesc)) +
  geom_point(size = 4, alpha = .5) +
  ylab('Proportion of Excellent response')
```

![](p8105_hw2_js5354_files/figure-markdown_github/questions%20related%20to%202002%20data-2.png)
